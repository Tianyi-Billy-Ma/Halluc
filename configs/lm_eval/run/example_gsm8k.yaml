# Example run configuration for evaluating GSM8K with a custom model
# Usage: python llmhalluc/run_eval.py --config configs/lm_eval/run/example_gsm8k.yaml

# Model configuration
model: "hf"
model_args: "pretrained=meta-llama/Llama-2-7b-hf,dtype=float16"

# Tasks to evaluate (can use custom tasks from configs/lm_eval/tasks/)
tasks: "gsm8k_custom"  # Uses the custom task defined in configs/lm_eval/tasks/gsm8k_custom.yaml

# Few-shot configuration (can override task default)
num_fewshot: 5

# Batch and device settings
batch_size: "auto"
device: "cuda:0"

# Output configuration
output_path: "results/gsm8k_eval"
log_samples: true

# Wandb logging (optional - remove or set to "" to disable)
wandb_args: "project=lm-eval-halluc,name=gsm8k-custom-eval"

# Reproducibility
seed: "42,42,42,42"

# Additional settings
apply_chat_template: false  # Set to true or template name for chat models
gen_kwargs: null  # Can override task's generation_kwargs if needed
