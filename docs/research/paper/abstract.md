**Title:** Autoregressive Sequence Modeling with Undo

**Abstract:**
Standard autoregressive models generate sequences monotonically, implying that any sampled token—even if erroneous or sub-optimal—becomes a permanent condition for all subsequent steps. This rigidity makes them highly susceptible to error propagation, where a single deviation causes the generation trajectory to drift irreversibly into low-probability regions. In this work, we propose **Autoregressive Sequence Modeling with Undo**, a framework that empowers models to dynamically revise their generation history via a native `<|BACKTRACK|>` token.

We introduce a novel training pipeline to operationalize this mechanism. First, we employ **Sequence Augmentation** over the error distribution, creating trajectories that demonstrate recovery from deviation. Crucially, we utilize **Masked Supervised Fine-Tuning (Masked SFT)**, where the loss is calculated solely on the backtracking and correction steps, effectively masking the error tokens. We show theoretically and empirically that this prevents the model from learning to generate errors while maximizing its capability to correct them. Subsequently, we refine the policy using **Group Relative Policy Optimization (GRPO)** with a multi-objective reward function that incentivizes outcome accuracy on the resolved sequence while strictly penalizing unnecessary or abundant backtracking.

Our experiments and comprehensive ablation studies validate the efficacy of this pipeline. We demonstrate that the "Undo" operator enables models to autonomously prune divergent trajectories, significantly improving performance on reasoning tasks. Furthermore, our results confirm that Masked SFT provides a superior initialization for the correction policy compared to standard SFT, establishing a robust foundation for efficiency-aware self-correction.
