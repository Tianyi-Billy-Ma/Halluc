# Batch generation configuration
# Usage: python -m llmhalluc.run_gen --config configs/llmhalluc/gen.yaml
#
# Multi-GPU: vLLM handles multi-GPU natively via tensor_parallel_size.
# Set tensor_parallel_size=N for N GPUs (must divide evenly into model layers).

run_name: gsm8k

stage: sft
finetuning_type: lora

# Model
model_name_or_path: meta-llama/Llama-3.2-1B
adapter_name_or_path: null
trust_remote_code: true

# Dataset (key from data/dataset_info.json)
dataset: gsm8k_train
max_samples: null

# Generation parameters
max_new_tokens: 512
temperature: 0.7
top_p: 0.9
top_k: -1
do_sample: true
num_return_sequences: 1

# vLLM settings (tensor_parallel_size: null = auto-detect GPUs)
tensor_parallel_size: null
gpu_memory_utilization: 0.9
max_model_len: null

# Output
output_filename: generations.jsonl
save_every: 100

# Misc
seed: 42
batch_size: 32
